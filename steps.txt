#First setup a repository and inside that setup yml file with docker compose 

version: "3.8"
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9001:9001"
      - "9002:9000"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"

  airflow-webserver:
    image: apache/airflow:2.9.3
    container_name: airflow-webserver
    restart: always
    depends_on:
      - airflow-scheduler
      - postgres
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.9.3
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: scheduler

  postgres:
    image: postgres:15
    container_name: airflow-postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:

#Next run the below command
docker compose up -d

#Next Run these commands
docker compose exec airflow-scheduler airflow db init
docker compose run airflow-webserver airflow db migrate
docker compose exec airflow-webserver airflow users create --username <<USER_NAME>> --firstname <<FIRST_NAME>> --lastname <<LAST_NAME>> --role Admin --email <<EMAIL>> --password password123

#Next specify the producer i.e; kafdrop ui create new Topic there  and consumer i.e; minion object store create S3 bucket and connect them
#Next inside the repository where our docker-compose ytml file is there inside that repository there exists a repository called as dags where you specify the dag and also provide the credentials for snowflake and see in the airflow webinterface.. task 1 download from minion and task 2 load to snowflake schedule in DAG
 
#Next create a folder called dbt_stocks and go inside it and run dbt init dbt_stocks so that a whole repo will be created and inside the models follow bronze,silver,gold nomenclature and create the ETL things to happen Trnasofrmations need to done and run dbt run command and also powerBI KPIS and views need to add later add a powerBI connection...

